from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import chromedriver_autoinstaller
import pandas as pd
import time
import re
import json
import os

# âœ… ChromeDriver ìë™ ì„¤ì¹˜
chromedriver_autoinstaller.install()

# âœ… Selenium ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless=new")  # âœ… ìµœì‹  headless ëª¨ë“œ ì ìš©
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-blink-features=AutomationControlled")
options.add_argument("--disable-gpu")  # âœ… GPU ë¹„í™œì„±í™” (ë Œë”ë§ ë¬¸ì œ ë°©ì§€)
options.add_argument("--disable-software-rasterizer")  # âœ… ë Œë”ë§ ë¬¸ì œ ë°©ì§€
options.add_argument("--log-level=3")  # âœ… ë¶ˆí•„ìš”í•œ ë¡œê·¸ ìˆ¨ê¸°ê¸°
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36")

# âœ… WebDriver ì‹¤í–‰
service = Service()
driver = webdriver.Chrome(service=service, options=options)
driver.implicitly_wait(5)  # ğŸ”¥ ìš”ì†Œ ë¡œë”©ì„ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ ì„¤ì •

def load_json(filename):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        print(f"ğŸš¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
        return []
    
def scroll_down(driver, scroll_container):
    last_height = driver.execute_script("return arguments[0].scrollHeight", scroll_container)
    while True:
        driver.execute_script("arguments[0].scrollTo(0, arguments[0].scrollHeight);", scroll_container)
        time.sleep(2)
        new_height = driver.execute_script("return arguments[0].scrollHeight", scroll_container)
        if new_height == last_height:
            break
        last_height = new_height

def extract_main_photo_url():
    """ìƒì„¸ ì •ë³´ í˜ì´ì§€ ë‚´ ë©”ì¸ ì‚¬ì§„ URL ì¶”ì¶œ"""
    try:
        main_photo_button = driver.find_element(By.CLASS_NAME, "main_photo_item")
        style_attribute = main_photo_button.get_attribute("style")
        match = re.search(r'url\("([^"]+)"\)', style_attribute)  # `url("...")` íŒ¨í„´ì—ì„œ URL ì¶”ì¶œ
        if match:
            return match.group(1)
        return ""
    except :
        return None

# âœ… í¬ë¡¤ë§í•  ì§€ì—­ ë¦¬ìŠ¤íŠ¸ (JSON íŒŒì¼ ë¡œë“œ)
locations = load_json("seoul_locations.json")

# âœ… ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
all_data = []
SCROLL_LIMIT = 3  # âœ… ìµœëŒ€ ìŠ¤í¬ë¡¤ ë‹¤ìš´ íšŸìˆ˜ ì„¤ì •

for location in locations:
    if location['ë²•ì •ë™ëª…'][:9] != "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬":
        continue
    print(f"ğŸ“Œ í¬ë¡¤ë§ ì¤‘: {location['ë²•ì •ë™ëª…']}")

    # âœ… í•´ë‹¹ ë²•ì •ë™ ì½”ë“œë¡œ ê²€ìƒ‰ëœ ë„¤ì´ë²„ ë¶€ë™ì‚° í˜ì´ì§€ ì ‘ì†
    url = f"https://new.land.naver.com/houses?cortarNo={location['ë²•ì •ë™ì½”ë“œ']}"
    driver.get(url)
    time.sleep(5)

    # âœ… ë§¤ ë™ë§ˆë‹¤ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ë‹¤ì‹œ ì°¾ìŒ
    for attempt in range(3):  # ğŸš¨ ìµœëŒ€ 3ë²ˆ ì‹œë„ (ìƒˆë¡œê³ ì¹¨ í¬í•¨)
        try:
            scroll_container = driver.find_element(By.CLASS_NAME, "item_list--article")
            break  # âœ… ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
        except:
            print(f"ğŸš¨ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, {attempt+1}ë²ˆì§¸ ì‹œë„ í›„ ìƒˆë¡œê³ ì¹¨")
            driver.refresh()
            time.sleep(5)

    # âœ… ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¡¤í•˜ì—¬ ì¶”ê°€ ë§¤ë¬¼ ë¡œë“œ
    last_count = 0
    scroll_count = 0

    while scroll_count < SCROLL_LIMIT:
        try:
            scroll_container = driver.find_element(By.CLASS_NAME, "item_list--article")
        except:
            print("ğŸš¨ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ë‹¤ì‹œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            break

        # í˜„ì¬ ë¡œë“œëœ ë§¤ë¬¼ ê°œìˆ˜ í™•ì¸
        listings = driver.find_elements(By.CLASS_NAME, "item_link")
        current_count = len(listings)

        # âœ… ìŠ¤í¬ë¡¤ ì‹¤í–‰
        scroll_down(driver, scroll_container)
        time.sleep(3)

        # âœ… ìƒˆë¡œìš´ ë§¤ë¬¼ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if current_count == last_count:
            print("ğŸš¨ ë” ì´ìƒ ë¡œë“œí•  ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            break

        last_count = current_count
        scroll_count += 1
        print(f"ğŸ”½ ìŠ¤í¬ë¡¤ ë‹¤ìš´ {scroll_count}/{SCROLL_LIMIT} - í˜„ì¬ ë§¤ë¬¼ ê°œìˆ˜: {current_count}")

    # âœ… ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    listings = driver.find_elements(By.CLASS_NAME, "item_link")
    print(f"âœ… ì´ {len(listings)}ê°œì˜ ë§¤ë¬¼ ë°œê²¬")

    if not listings:
        print(f"ğŸš¨ {location['ë²•ì •ë™ëª…']} - ë§¤ë¬¼ ì—†ìŒ")
        continue

    # âœ… ë§¤ë¬¼ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
    for listing in listings:
        try:
            listing.click()
            time.sleep(2)

            # âœ… í˜„ì¬ URLì—ì„œ articleNo ì¶”ì¶œ
            current_url = driver.current_url
            match = re.search(r"articleNo=(\d+)", current_url)
            if not match:
                print("ğŸš¨ ë§¤ë¬¼ ë²ˆí˜¸(articleNo) ì¶”ì¶œ ì‹¤íŒ¨")
                continue

            article_no = match.group(1)
            detail_url = f"https://new.land.naver.com/houses?articleNo={article_no}"

            # âœ… ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            try:
                ìœ í˜• = driver.find_element(By.CLASS_NAME, "type").text.strip()
            except:
                ìœ í˜• = ""

            try:
                ê±°ë˜ìœ í˜• = driver.find_element(By.XPATH, "//div[contains(@class, 'info_article_price')]/span[1]").text.strip()
            except:
                ê±°ë˜ìœ í˜• = ""

            try:
                ê°€ê²© = driver.find_element(By.XPATH, "//div[contains(@class, 'info_article_price')]//span[contains(@class, 'price')]").text.strip()
            except:
                ê°€ê²© = ""

            try:
                ì†Œì¬ì§€ = driver.find_element(By.XPATH, "//th[contains(text(), 'ì†Œì¬ì§€')]/following-sibling::td").text.strip()
            except:
                ì†Œì¬ì§€ = ""

            try:
                ë§¤ë¬¼íŠ¹ì§• = driver.find_element(By.XPATH, "//th[contains(text(), 'ë§¤ë¬¼íŠ¹ì§•')]/following-sibling::td").text.strip()
            except:
                ë§¤ë¬¼íŠ¹ì§• = ""

            try:
                ë©´ì  = driver.find_element(By.XPATH, "//th[contains(text(), 'ê³µê¸‰/ì „ìš©ë©´ì ')]/following-sibling::td").text.strip()
            except:
                ë©´ì  = ""

            try:
                ì¸µìˆ˜ = driver.find_element(By.XPATH, "//th[contains(text(), 'í•´ë‹¹ì¸µ/ì´ì¸µ')]/following-sibling::td").text.strip()
            except:
                ì¸µìˆ˜ = ""

            try:
                ë°©ìš•ì‹¤ìˆ˜ = driver.find_element(By.XPATH, "//th[contains(text(), 'ë°©ìˆ˜/ìš•ì‹¤ìˆ˜')]/following-sibling::td").text.strip()
            except:
                ë°©ìš•ì‹¤ìˆ˜ = ""

            try:
                ê´€ë¦¬ë¹„ = driver.find_element(By.XPATH, "//th[contains(text(), 'ê´€ë¦¬ë¹„')]/following-sibling::td").text.strip()
            except:
                ê´€ë¦¬ë¹„ = ""

            try:
                ì…ì£¼ê°€ëŠ¥ = driver.find_element(By.XPATH, "//th[contains(text(), 'ì…ì£¼ê°€ëŠ¥ì¼')]/following-sibling::td").text.strip()
            except:
                ì…ì£¼ê°€ëŠ¥ = ""

            try:
                ë°©í–¥ = driver.find_element(By.XPATH, "//th[contains(text(), 'ë°©í–¥')]/following-sibling::td").text.strip()
            except:
                ë°©í–¥ = ""

            # âœ… ë©”ì¸ ì‚¬ì§„ URL ê°€ì ¸ì˜¤ê¸°
            main_photo_url = extract_main_photo_url()

            # âœ… ë°ì´í„° ì €ì¥
            all_data.append({
                "ë§¤ë¬¼ë²ˆí˜¸": article_no,
                "ìœ í˜•": ìœ í˜•,
                "ê±°ë˜ìœ í˜•": ê±°ë˜ìœ í˜•,
                "ê°€ê²©": ê°€ê²©,
                "ì†Œì¬ì§€": ì†Œì¬ì§€,
                "ë§¤ë¬¼íŠ¹ì§•": ë§¤ë¬¼íŠ¹ì§•,
                "ê³µê¸‰/ì „ìš©ë©´ì ": ë©´ì ,
                "í•´ë‹¹ì¸µ/ì´ì¸µ": ì¸µìˆ˜,
                "ë°©ìˆ˜/ìš•ì‹¤ìˆ˜": ë°©ìš•ì‹¤ìˆ˜,
                "ê´€ë¦¬ë¹„": ê´€ë¦¬ë¹„,
                "ì…ì£¼ ê°€ëŠ¥ì¼": ì…ì£¼ê°€ëŠ¥,
                "ë°©í–¥": ë°©í–¥,
                "URL": detail_url,
                "ë©”ì¸ì‚¬ì§„": main_photo_url
            })

            print(f"âœ… ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {ì†Œì¬ì§€} - {ê°€ê²©} - ì‚¬ì§„: {main_photo_url}")

            time.sleep(2)

        except Exception as e:
            print(f"ğŸš¨ ìƒì„¸ í˜ì´ì§€ ì´ë™ ì˜¤ë¥˜: {e}")

    print(f"ğŸ“Œ {location['ë²•ì •ë™ëª…']} í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_data)}ê°œ ë§¤ë¬¼ ìˆ˜ì§‘.")

    # âœ… í˜ì´ì§€ ë‹¤ì‹œ ë¡œë“œ
    driver.get(url)
    time.sleep(5)

# âœ… JSON íŒŒì¼ë¡œ ì €ì¥
if all_data:
    with open("naver_real_estate_details.json", "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)
    print("âœ… ëª¨ë“  ë§¤ë¬¼ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ! JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
else:
    print("ğŸš¨ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# âœ… WebDriver ì¢…ë£Œ
driver.quit()
